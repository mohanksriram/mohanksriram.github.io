<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Ghost]]></title><description><![CDATA[The professional publishing platform]]></description><link>https://gatsby-casper.netlify.com</link><generator>RSS for Node</generator><lastBuildDate>Thu, 20 Jun 2019 09:19:27 GMT</lastBuildDate><item><title><![CDATA[Model-Agnostic Meta-Learning]]></title><description><![CDATA[Welcome to this first post on meta learning! This post assumes that the reader is already familiar with the basic neural network‚Ä¶]]></description><link>https://gatsby-casper.netlify.com/maml/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/maml/</guid><pubDate>Sun, 16 Jun 2019 10:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Welcome to this first post on meta learning! This post assumes that the reader is already familiar with the basic neural network architectures and their training processes. Assuming you are ready, let‚Äôs begin!&lt;/p&gt;
&lt;p&gt;&lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/cd030ee7e4e0c904d12eb69dfedd0e19/97529/learn-to-learn.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 1000px;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 20%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAEABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB2QWD/8QAFxAAAwEAAAAAAAAAAAAAAAAAAAIyQf/aAAgBAQABBQI1J//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABUQAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAEBAAY/Am//xAAZEAEAAwEBAAAAAAAAAAAAAAABABExIWH/2gAIAQEAAT8hcFuzU+VBXE//2gAMAwEAAgADAAAAEIAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGRABAQEBAQEAAAAAAAAAAAAAAREhADFR/9oACAEBAAE/EKcYlo6caS36M84ABN3be//Z&apos;); background-size: cover; display: block;&quot;
    &gt;&lt;/span&gt;
    &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;&quot;
        alt=&quot;logo&quot;
        title=&quot;&quot;
        src=&quot;/static/cd030ee7e4e0c904d12eb69dfedd0e19/97529/learn-to-learn.jpg&quot;
        srcset=&quot;/static/cd030ee7e4e0c904d12eb69dfedd0e19/e33f8/learn-to-learn.jpg 293w,
/static/cd030ee7e4e0c904d12eb69dfedd0e19/d5ce0/learn-to-learn.jpg 585w,
/static/cd030ee7e4e0c904d12eb69dfedd0e19/97529/learn-to-learn.jpg 1000w&quot;
        sizes=&quot;(max-width: 1000px) 100vw, 1000px&quot;
      /&gt;
  &lt;/span&gt;
  &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Humans are the ultimate adaptation machines. We can act and adapt intelligently to a wide range of new, unseen situations and tasks at an incredibly fast pace. Although there has been a huge hype about Artificial General Intelligence, the current state of the art AI systems are far from versatile. Many of the recent achievements &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt;, &lt;a href=&quot;https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/&quot;&gt;Alpha-Zero&lt;/a&gt;, and &lt;a href=&quot;https://openai.com/blog/learning-dexterity/&quot;&gt;Dractyl&lt;/a&gt;, fall into a category of systems which have perfected a narrow skill with thousands of hours of training on huge GPU clusters using millions of data-points. These systems can struggle to perform seemingly simple tasks such as turning door knobs which they have not seen before.   &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/door-knob1-7eabf5b6a854ed2538ed1b00701cc0ed.gif&quot; alt=&quot;DoorKnob1&quot;&gt;&lt;/p&gt;
&lt;p&gt;An alternative approach could be to build purely control systems based machines with fully defined state spaces for a few selective manouvers, such as &lt;a href=&quot;https://www.bostondynamics.com/&quot;&gt;Boston Dynamics&lt;/a&gt;‚Äôs quirky &lt;a href=&quot;https://www.bostondynamics.com/spot&quot;&gt;Spot&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/door-knob2-0dcac3fb9769bf3946935b552450318e.gif&quot; alt=&quot;DoorKnob2&quot;&gt;&lt;/p&gt;
&lt;p&gt;But if we are to come anywhere close to human-level adaptability, we need our agents to not only learn a particular skill from scratch (in isolation) but more importantly learn how to learn new tasks by resusing experience from past tasks. As you might have expected, we have a bunch of brilliant minds already working on this hard problem! A new sub-field in artificial intelligence called ‚ÄòMeta-Learning‚Äô or ‚ÄòLearning to Learn‚Äô has emerged with the aim of learning to perform a wide range of tasks with sparse data.&lt;/p&gt;
&lt;p&gt;You can find the current state of the art meta learning approaches below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prototypical-Nets&lt;/li&gt;
&lt;li&gt;MAML&lt;/li&gt;
&lt;li&gt;Memory Mod&lt;/li&gt;
&lt;li&gt;Neural Statistician&lt;/li&gt;
&lt;li&gt;Matching Nets&lt;/li&gt;
&lt;li&gt;Reptile + Transduction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this blog post, we will explore one of the learning to learn approaches called ‚ÄúModel-Agnostic Meta-Learning‚Äù which is simple yet effective. ‚ÄúModel-Agnostic‚Äù because the approach works for multiple domains including supervised regression, classification and reinforcement learning. We will discuss the intuition, core algorithm, and also link to an implementation. &lt;/p&gt;
&lt;h3&gt;Intuition&lt;/h3&gt;
&lt;p&gt;&lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/56d73bc68b38b2a472f6a48c518202ca/fc87f/maml-learner.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 552px;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 72.46376811594205%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAAB6klEQVQ4y5WT227aQBCG8/6v0ZteRZXaRKl6kRS1hJ5kCA1g4gDBRwxe2+u11+e/uy5QTFHSjjTyyp79/M/pDMIYYwgDX5wqEBqjdzeFMtURxVx+Rl3Xz/phzBm2ZpEYmuVjOHehLmwElIGnKfI8x0u2g0prgGVVoTsycPFVw4e+jplN9gGcc5Rl2YDX6zV83weNIvC8RJzmyIuqrXBHj3gBj3JswgRpXrQUpEKpdEI8+MI9QvBjNMNnRYXuBpKGegttFCZJAm+zRsqTRm3IOMI4RSGU7SxiMaYOw9RNMbYi3OsBhssAmukhz7K9ygaYiReeR5rmGK6P6y93UNQlKEuQCWWFSJdGDO8+DfDqbQevr7qNX33ToJq+AKZtoDyMDR+3ExvdsYWF7YmGl/ugSqhOeIbL7k+c3/RxcTvGm84AH+8dPNhUBFXtlOUFRbNx05+jM1ziaRU0sOqgeyxOMHNCzF2G2Yo2T82hcAiTKFnGdlP+jEB7JOTPdp3Gbx1/eX0wOo3CSIyBaRhgERWAGubKg+mKmiayu/xoeHHC6+MaVoh5ikTMleNR9IaPUCYL+NF2U17YltYc7uqkGgTvRdeuB0tM9M0Wg5OXnrN9DZ9WIXojHd8fHDxapL3DR+v1T0CZ9ilF+A+YtF9ZJDpmN8g/gwAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;&lt;/span&gt;
    &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;&quot;
        alt=&quot;model&quot;
        title=&quot;&quot;
        src=&quot;/static/56d73bc68b38b2a472f6a48c518202ca/fc87f/maml-learner.png&quot;
        srcset=&quot;/static/56d73bc68b38b2a472f6a48c518202ca/bc34b/maml-learner.png 293w,
/static/56d73bc68b38b2a472f6a48c518202ca/fc87f/maml-learner.png 552w&quot;
        sizes=&quot;(max-width: 552px) 100vw, 552px&quot;
      /&gt;
  &lt;/span&gt;
  &lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;center&gt;Fig 1&lt;/center&gt;&lt;/h5&gt;
&lt;p&gt;The Main intuition behind the MAML algorithm is depicted in Fig 1.&lt;/p&gt;
&lt;p&gt;In every iteration of the training process, we begin by finding the optimal parameters(weights) of the model for each of the tasks individually through independent gradient descents (equation in Fig 2). We obtain a set of optimal parameters [ùúÉbar], these parameters are used to compute the optimal parameter for the meta learner (ùúÉ) by performing another gradient descent step (equation in Fig 3). &lt;/p&gt;
&lt;p&gt;Note: Optimal parameter (ùúÉ) corresponds to all the weights required for the model.&lt;/p&gt;
&lt;h3&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;&lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/d5035d0019410dfec3bf3ee9f2b561fd/1c926/fig2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 408px;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 23.52941176470588%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAAA0UlEQVQY031Qy3KDMBDj/3+q8BHJLbRwIHbjN7EDFFBkHzppMo1mNNbs2mtpq33f8cgM7xzOQkCQLuthQN/3EFJioM71bdvw/DazwgPWdcU8z7/8WRbWNky3G7z3ZVAIATEm/IdK8tJn2yKlhHgdcTgcizbGQMpvWJ4Th1ujcTq11Au0utAte3Qf+NE1RibxWGigUkqh6zqsjCDOAz7qGlobjOOI3FNaF+d994W6aeB84EosrLUwpLOm3M8G8hr+RE4psqkxTfNLlEgXSuni4h3up6eBN7hcvDEAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
    &gt;&lt;/span&gt;
    &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Fig2&quot;
        title=&quot;&quot;
        src=&quot;/static/d5035d0019410dfec3bf3ee9f2b561fd/1c926/fig2.png&quot;
        srcset=&quot;/static/d5035d0019410dfec3bf3ee9f2b561fd/bc34b/fig2.png 293w,
/static/d5035d0019410dfec3bf3ee9f2b561fd/1c926/fig2.png 408w&quot;
        sizes=&quot;(max-width: 408px) 100vw, 408px&quot;
      /&gt;
  &lt;/span&gt;
  &lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;center&gt;Fig 2&lt;/center&gt;&lt;/h5&gt;
&lt;h3&gt;Training Process&lt;/h3&gt;
&lt;p&gt;&lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/1b9d87dca4f10badaa3f7de577457860/1fae5/algo.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 419px;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 67.06443914081146%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAACHklEQVQ4y3VTi26qUBD0///pGmt6E7FX8S3ykDdoRUEUC4pOdzchsbn2JBtgOcyZmR1at9sNfUWBovTRbv/BdDrFcKhCHQ6hrVaY0fN8PsdsPoOqjrCi3nQyoT0DuK6Lw+GAJEmw2+1wPp/RKssSf9/f0ev1MKaNge/DNC2sLRO6oWM8HsOyLPjUt+01VrqB7XaL/X6P4/GIuq5xvV6l+L5VEGr3rYNOpw3DsnHKc6RZhjDwYRgmHMfBcrGAaRhyELP1PB+3Ww1ej8cDz6t1v98RhQE0bUmn6yTDQRRHBKCLZGacJHtk6YHkpcjosDTNwMoawOdqcbOqSpHV7b4JK9u2Sa6B0WiEy+WC31bD7j/A0+mEnPxYLomlpgk79opl+kFA8gPxrCgu2GxiYfqK3Q/AiiQwGE/bdlya9kT6LI2nx2lg08uvL7BNvy0BZF9q2pQfMyyIJctfU8VxTJPMKBKJRIP93Wy2cB32OcYnRWW3+5T46OR/Qfa0iKcAsh11fRP/OCq+H0hMRqoqQAbJZ0v4wyiK5NlzPVGlUX+9Xks6BJCl8GJ5pslDcSSoXGmaClOHemEYEliIsqpENmevovvGDu6J5HNRSGM+m+Hjo08AG3nJG/nKB/HHr4bwwsOHsGDjmf5g8A8OecIT5T+B+57nUVZDec7zo0w8oYpJOu/7Acin8DQtkspBVpQeAXryktk1xWxZXsOWFT1Lba7fawDR/KVJbY0AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
    &gt;&lt;/span&gt;
    &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;&quot;
        alt=&quot;algo&quot;
        title=&quot;&quot;
        src=&quot;/static/1b9d87dca4f10badaa3f7de577457860/1fae5/algo.png&quot;
        srcset=&quot;/static/1b9d87dca4f10badaa3f7de577457860/bc34b/algo.png 293w,
/static/1b9d87dca4f10badaa3f7de577457860/1fae5/algo.png 419w&quot;
        sizes=&quot;(max-width: 419px) 100vw, 419px&quot;
      /&gt;
  &lt;/span&gt;
  &lt;/a&gt;&lt;/p&gt;
&lt;h5&gt;&lt;center&gt;Fig 3&lt;/center&gt;&lt;/h5&gt;
&lt;p&gt;Overall, MAML algorithm produces a weight initialization that results in easily adaptable model parameters. This allows the model to perform well on new tasks with few examples.&lt;/p&gt;
&lt;p&gt;To get up and running with MAML, please refer to &lt;a href=&quot;https://github.com/mohankumarSriram/maml.git&quot;&gt;this&lt;/a&gt; Pytorch implementation of the algorithm which works with the latest versions of Python and Pytorch.&lt;/p&gt;
&lt;p&gt;Note: My Implementation is adapted from the original implementation &lt;a href=&quot;https://github.com/katerakelly/pytorch-maml&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On the next post, we will explore the application of MAML in a reiforcement learning setting.&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Chelsea Finn, Pieter Abbeel, and Sergey Levine. &lt;a href=&quot;https://arxiv.org/abs/1703.03400&quot;&gt;‚ÄúModel-agnostic meta-learning for fast adaptation of deep networks.‚Äù&lt;/a&gt; ICML 2017&lt;/li&gt;
&lt;li&gt;Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. &lt;a href=&quot;https://science.sciencemag.org/content/350/6266/1332.full&quot;&gt;‚ÄúHuman-level concept learning through probabilistic program induction.‚Äù&lt;/a&gt; Science 350.6266 (2015): 1332-1338.&lt;/li&gt;
&lt;li&gt;Chelsea Finn‚Äôs BAIR blog on &lt;a href=&quot;https://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/&quot;&gt;Learning to Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Siavash Khodadadeh‚Äôs lecture on &lt;a href=&quot;https://youtu.be/wT45v8sIMDM&quot;&gt;MAML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Papers with code: &lt;a href=&quot;https://www.paperswithcode.com/sota/few-shot-image-classification-on-omniglot-1&quot;&gt;Few shot learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item></channel></rss>